{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18.2M/18.2M [00:00<00:00, 373MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software version: VD/VE (!?)\n",
      "\n",
      "Scan  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 37.1M/8.92G [00:00<00:24, 387MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8.92G/8.92G [00:22<00:00, 425MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spirals: 48\n",
      "Measured Partitions: 30\n",
      "Undersampled Partitions: 60\n",
      "Center Measured Partition: 15\n",
      "Sets: 20\n",
      "Coils: 18\n",
      "Matrix Size: [256 256  60]\n",
      "Undersampling Ratio: 2\n",
      "Discard Pre: 20\n",
      "Discard Post: 1936\n",
      "Readout Points: 1916\n",
      "Dictionary Parameter set '5pct' initialized with 12504 entries\n",
      "TRs: 0.0 10500.0\n",
      "TEs: 1680.0 1680.0\n",
      "FAs: 0.0 5700.0\n",
      "IDs: 0.0 47.0\n",
      "Sequence Parameter set 'largescale' initialized with 960 timepoint definitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:10<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD Compressed Raw Data Shape: torch.Size([10, 18, 60, 1916, 48])\n",
      "Found (48, 1916) spirals\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import twixtools\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter import filedialog as fd\n",
    "from tkinter.messagebox import showinfo\n",
    "import numpy as np\n",
    "import ismrmrd\n",
    "\n",
    "import ismrmrd\n",
    "import os\n",
    "import itertools\n",
    "import logging\n",
    "import numpy as np\n",
    "import numpy.fft as fft\n",
    "import ctypes\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "from mrftools import *\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "import time\n",
    " \n",
    "\n",
    "# Folder for debug output files\n",
    "dictionaryFolder = \"\"\n",
    "\n",
    "# Configure dictionary simulation parameters\n",
    "dictionaryName = \"5pct\"\n",
    "percentStepSize=5; includeB1=False;  t1Range=(10,4000); t2Range=(1,500); b1Range=(0.5, 1.55); b1Stepsize=0.05; \n",
    "phaseRange=(-np.pi, np.pi); numSpins=15; numBatches=100\n",
    "\n",
    "# Azure logging configuration (temporary for testing, should be a secret in the cluster not plaintext)\n",
    "connectionString = \"\"\n",
    "tableName = \"reconstructionLog\"\n",
    "\n",
    "def ApplyXYZShift(svdData, header, acqHeaders, trajectories, matrixSizeOverride=None):\n",
    "    shape = np.shape(svdData)\n",
    "    numSVDComponents=shape[0]; numCoils=shape[1]; numPartitions=shape[2]; numReadoutPoints=shape[3]; numSpirals=shape[4]\n",
    "    shiftedSvdData = torch.zeros_like(svdData)\n",
    "    # For now, assume all spirals/partitions/etc have same offsets applied\n",
    "    (x_shift, y_shift, z_shift) = CalculateVoxelOffsetAcquisitionSpace(header, acqHeaders[0,0,0], matrixSizeOverride=matrixSizeOverride)\n",
    "    trajectories = torch.t(torch.tensor(np.array(trajectories)))\n",
    "    x = torch.zeros((numPartitions, numReadoutPoints, numSpirals));\n",
    "    y = torch.zeros((numPartitions, numReadoutPoints, numSpirals));\n",
    "    partitions = torch.moveaxis(torch.arange(-0.5, 0.5, 1/numPartitions).expand((numReadoutPoints, numSpirals, numPartitions)), -1,0)\n",
    "    trajectories = trajectories.expand((numPartitions, numReadoutPoints, numSpirals))\n",
    "    x = torch.cos(-2*torch.pi*(x_shift*trajectories.real + y_shift*trajectories.imag + z_shift*partitions));\n",
    "    y = torch.sin(-2*torch.pi*(x_shift*trajectories.real + y_shift*trajectories.imag + z_shift*partitions));\n",
    "    logging.info(f\"K-Space x/y/z shift applied: {x_shift}, {y_shift}, {z_shift}\")\n",
    "    return svdData*torch.complex(x,y)\n",
    "\n",
    "def vertex_of_parabola(points, clamp=False, min=None, max=None):\n",
    "    x1 = points[:,0,0]\n",
    "    y1 = points[:,0,1]\n",
    "    x2 = points[:,1,0]\n",
    "    y2 = points[:,1,1]\n",
    "    x3 = points[:,2,0]\n",
    "    y3 = points[:,2,1]\n",
    "    denom = (x1-x2) * (x1-x3) * (x2-x3)\n",
    "    A = (x3 * (y2-y1) + x2 * (y1-y3) + x1 * (y3-y2)) / denom\n",
    "    B = (x3*x3 * (y1-y2) + x2*x2 * (y3-y1) + x1*x1 * (y2-y3)) / denom\n",
    "    C = (x2 * x3 * (x2-x3) * y1+x3 * x1 * (x3-x1) * y2+x1 * x2 * (x1-x2) * y3) / denom\n",
    "    xv = -B / (2*A)\n",
    "    yv = C - B*B / (4*A)\n",
    "    if clamp:\n",
    "        torch.clamp(xv, min, max)\n",
    "    return (xv, yv)\n",
    "\n",
    "def GenerateDictionaryLookupTables(dictionaryEntries):\n",
    "    uniqueT1s = np.unique(dictionaryEntries['T1'])\n",
    "    uniqueT2s = np.unique(dictionaryEntries['T2'])\n",
    "\n",
    "    dictionary2DIndexLookupTable = []\n",
    "    dictionaryEntries2D = np.zeros((len(uniqueT1s), len(uniqueT2s)), dtype=DictionaryEntry)\n",
    "    dictionary1DIndexLookupTable = np.zeros((len(uniqueT1s), len(uniqueT2s)), dtype=int)\n",
    "    for dictionaryIndex in tqdm(range(len(dictionaryEntries))):\n",
    "        entry = dictionaryEntries[dictionaryIndex]\n",
    "        T1index = np.where(uniqueT1s == entry['T1'])[0]\n",
    "        T2index = np.where(uniqueT2s == entry['T2'])[0]\n",
    "        dictionaryEntries2D[T1index, T2index] = entry\n",
    "        dictionary1DIndexLookupTable[T1index, T2index] = dictionaryIndex\n",
    "        dictionary2DIndexLookupTable.append([T1index,T2index])\n",
    "    dictionary2DIndexLookupTable = np.array(dictionary2DIndexLookupTable)\n",
    "    return uniqueT1s, uniqueT2s, dictionary1DIndexLookupTable, dictionary2DIndexLookupTable\n",
    "\n",
    "\n",
    "def BatchPatternMatchViaMaxInnerProductWithInterpolation(signalTimecourses, dictionaryEntries, dictionaryEntryTimecourses, voxelsPerBatch=500, device=None, radius=1):\n",
    "    if(device==None):\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        uniqueT1s, uniqueT2s, dictionary1DIndexLookupTable, dictionary2DIndexLookupTable = GenerateDictionaryLookupTables(dictionaryEntries)\n",
    "\n",
    "        signalsTransposed = torch.t(signalTimecourses)\n",
    "        signalNorm = torch.linalg.norm(signalsTransposed, axis=1)[:,None]\n",
    "        normalizedSignals = signalsTransposed / signalNorm\n",
    "\n",
    "        simulationResults = torch.tensor(dictionaryEntryTimecourses, dtype=torch.complex64)\n",
    "        simulationNorm = torch.linalg.norm(simulationResults, axis=0)\n",
    "        normalizedSimulationResults = torch.t((simulationResults / simulationNorm)).to(device)\n",
    "\n",
    "        numBatches = int(np.shape(normalizedSignals)[0]/voxelsPerBatch)\n",
    "        patternMatches = np.empty((np.shape(normalizedSignals)[0]), dtype=DictionaryEntry)\n",
    "        interpolatedMatches = np.empty((np.shape(normalizedSignals)[0]), dtype=DictionaryEntry)\n",
    "\n",
    "        offsets = np.mgrid[-1*radius:radius+1, -1*radius:radius+1]\n",
    "        numNeighbors = np.shape(offsets)[1]*np.shape(offsets)[2]\n",
    "        \n",
    "        M0 = torch.zeros(np.shape(normalizedSignals)[0], dtype=torch.complex64)\n",
    "        with tqdm(total=numBatches) as pbar:\n",
    "            for i in range(numBatches):\n",
    "                firstVoxel = i*voxelsPerBatch\n",
    "                if i == (numBatches-1):\n",
    "                    lastVoxel = np.shape(normalizedSignals)[0]\n",
    "                else:\n",
    "                    lastVoxel = firstVoxel+voxelsPerBatch\n",
    "                batchSignals = normalizedSignals[firstVoxel:lastVoxel,:].to(device)\n",
    "                innerProducts = torch.inner(batchSignals, normalizedSimulationResults)\n",
    "                maxInnerProductIndices = torch.argmax(torch.abs(innerProducts), 1, keepdim=True)\n",
    "                maxInnerProducts = torch.take_along_dim(innerProducts,maxInnerProductIndices,dim=1).squeeze()\n",
    "                signalNorm_device = signalNorm[firstVoxel:lastVoxel].squeeze().to(device)\n",
    "                simulationNorm_device = simulationNorm.to(device)[maxInnerProductIndices.squeeze().to(torch.long)]\n",
    "                M0_device = signalNorm_device/simulationNorm_device\n",
    "                M0[firstVoxel:lastVoxel] = M0_device.cpu()\n",
    "                patternValues = dictionaryEntries[maxInnerProductIndices.squeeze().to(torch.long).cpu()].squeeze()\n",
    "                patternMatches[firstVoxel:lastVoxel] = patternValues\n",
    "                \n",
    "                indices = dictionary2DIndexLookupTable[maxInnerProductIndices.squeeze().to(torch.long).cpu()].squeeze()\n",
    "\n",
    "                numVoxels = len(maxInnerProductIndices)\n",
    "                neighbor2DIndices = np.reshape(indices.repeat(numNeighbors,axis=1),(np.shape(indices)[0], np.shape(indices)[1],np.shape(offsets)[1], np.shape(offsets)[2])) + offsets\n",
    "                neighbor2DIndices[:,0,:,:] = np.clip(neighbor2DIndices[:,0,:,:], 0, np.shape(dictionary1DIndexLookupTable)[0]-1)\n",
    "                neighbor2DIndices[:,1,:,:] = np.clip(neighbor2DIndices[:,1,:,:], 0, np.shape(dictionary1DIndexLookupTable)[1]-1)\n",
    "\n",
    "                neighborDictionaryIndices = torch.tensor(dictionary1DIndexLookupTable[neighbor2DIndices[:,0,:,:], neighbor2DIndices[:,1,:,:]].reshape(numVoxels, -1)).to(device)\n",
    "                neighborInnerProducts = torch.take_along_dim(torch.abs(innerProducts),neighborDictionaryIndices,dim=1).squeeze()\n",
    "                neighborDictionaryEntries = dictionaryEntries[neighborDictionaryIndices.cpu()].squeeze()\n",
    "\n",
    "                #Sum of inner products through T2 neighbors for each T1 neighbor\n",
    "                T1InnerProductSums = torch.stack((torch.sum(neighborInnerProducts[:, [0,1,2]], axis=1), torch.sum(neighborInnerProducts[:, [3,4,5]], axis=1), torch.sum(neighborInnerProducts[:,[6,7,8]], axis=1))).t()\n",
    "                T2InnerProductSums = torch.stack((torch.sum(neighborInnerProducts[:, [0,3,6]], axis=1), torch.sum(neighborInnerProducts[:,[1,4,7]], axis=1), torch.sum(neighborInnerProducts[:,[2,5,8]], axis=1))).t()\n",
    "\n",
    "                T1s = torch.tensor(neighborDictionaryEntries['T1'][:, [0,3,6]]).to(device)\n",
    "                stacked_T1 = torch.stack((T1s, T1InnerProductSums))\n",
    "                stacked_T1 = torch.moveaxis(stacked_T1, 0,1)\n",
    "\n",
    "                T2s = torch.tensor(neighborDictionaryEntries['T2'][:, [0,1,2]]).to(device)\n",
    "                stacked_T2 = torch.stack((T2s, T2InnerProductSums))\n",
    "                stacked_T2 = torch.moveaxis(stacked_T2, 0,1)\n",
    "\n",
    "                interpolatedValues = np.zeros((numVoxels),dtype=DictionaryEntry)\n",
    "                interpT1s, _ = vertex_of_parabola(torch.moveaxis(stacked_T1,1,2), clamp=True, min=0, max=np.max(uniqueT1s))\n",
    "                interpT2s, _ = vertex_of_parabola(torch.moveaxis(stacked_T2,1,2), clamp=True, min=0, max=np.max(uniqueT2s))\n",
    "                \n",
    "                interpolatedValues['T1'] = interpT1s.cpu()\n",
    "                interpolatedValues['T2'] = interpT2s.cpu()\n",
    "                interpolatedValues['B1'] = 1\n",
    "                \n",
    "                # For \"edge\" voxels, replace the interpolated values with the original pattern matches\n",
    "                edgeT1s = (indices[:,0] == (len(uniqueT1s)-1)) + (indices[:,0] == (0))\n",
    "                interpolatedValues[edgeT1s] = patternValues[edgeT1s]\n",
    "                \n",
    "                # For \"edge\" voxels, replace the interpolated values with the original pattern matches\n",
    "                edgeT2s = (indices[:,1] == (len(uniqueT2s)-1)) + (indices[:,1] == (0))\n",
    "                interpolatedValues[edgeT2s] = patternValues[edgeT2s]\n",
    "                \n",
    "                # For \"nan\" voxels, replace the interpolated values with the original pattern matches\n",
    "                nanT1s = np.isnan(interpolatedValues['T1'])\n",
    "                interpolatedValues[nanT1s] = patternValues[nanT1s]\n",
    "\n",
    "                # For \"nan\" voxels, replace the interpolated values with the original pattern matches\n",
    "                nanT2s = np.isnan(interpolatedValues['T2'])\n",
    "                interpolatedValues[nanT2s] = patternValues[nanT2s]\n",
    "                \n",
    "                interpolatedMatches[firstVoxel:lastVoxel] = interpolatedValues\n",
    "                pbar.update(1)\n",
    "                del batchSignals, M0_device, signalNorm_device, simulationNorm_device\n",
    "\n",
    "        del normalizedSimulationResults, dictionaryEntryTimecourses, dictionaryEntries, signalsTransposed, signalNorm, normalizedSignals, simulationResults\n",
    "        del simulationNorm\n",
    "        return patternMatches,interpolatedMatches, M0\n",
    "\n",
    "def AddText(image, text=\"NOT FOR DIAGNOSTIC USE\", fontSize=12):\n",
    "    matrixsize = np.shape(image)\n",
    "    img = Image.fromarray(np.uint8(np.zeros((matrixsize[0:2]))))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.truetype(\"terminess.ttf\", fontSize)\n",
    "    _, _, w, h = draw.textbbox((0, 0), text, font=font)\n",
    "    draw.text(((matrixsize[0]-w)/2, h/2), text, (255), font=font)\n",
    "    overlay = np.array(img) > 0\n",
    "    repeated = np.repeat(overlay[:,:,np.newaxis], matrixsize[2],axis=2)\n",
    "    repeated = repeated + np.rot90(repeated)\n",
    "    repeated = repeated + np.rot90(repeated)    \n",
    "    repeated = repeated + np.rot90(repeated)    \n",
    "    image[repeated] = np.max(image)\n",
    "    return image\n",
    "\n",
    "def LoadB1Map(matrixSize, b1Filename, resampleToMRFMatrixSize=True, deinterleave=True, deleteB1File=True):\n",
    "    # Using header, generate a unique b1 filename. This is temporary\n",
    "    try:\n",
    "        b1Data = np.load(b1Folder + \"/\" + b1Filename +\".npy\")\n",
    "    except:\n",
    "        logging.info(\"No B1 map found with requested filename. Trying fallback. \")\n",
    "        try:\n",
    "            b1Filename = f\"B1Map_fallback\"\n",
    "            b1Data = np.load(b1Folder + \"/\" + b1Filename +\".npy\")\n",
    "        except:\n",
    "            logging.info(\"No B1 map found with fallback filename. Skipping B1 correction.\")\n",
    "            return np.array([])\n",
    "\n",
    "    b1MapSize = np.array(np.shape(b1Data))\n",
    "    logging.info(f\"B1 Input Size: {b1MapSize}\")\n",
    "    if deinterleave:\n",
    "        numSlices = b1MapSize[2]\n",
    "        deinterleaved = np.zeros_like(b1Data)\n",
    "        deinterleaved[:,:,np.arange(1,numSlices,2)] = b1Data[:,:,0:int(np.floor(numSlices/2))]\n",
    "        deinterleaved[:,:,np.arange(0,numSlices-1,2)] = b1Data[:,:,int(np.floor(numSlices/2)):numSlices]\n",
    "        b1Data = deinterleaved\n",
    "    if resampleToMRFMatrixSize:\n",
    "        b1Data = scipy.ndimage.zoom(b1Data, matrixSize/b1MapSize, order=5)\n",
    "        b1Data = np.flip(b1Data, axis=2)\n",
    "        b1Data = np.rot90(b1Data, axes=(0,1))\n",
    "        b1Data = np.flip(b1Data, axis=0)\n",
    "    logging.info(f\"B1 Output Size: {np.shape(b1Data)}\")\n",
    "    if(deleteB1File):\n",
    "        os.remove(b1Folder + \"/\" + b1Filename +\".npy\")     \n",
    "        logging.info(f\"Deleted B1 File: {b1Filename}\")\n",
    "    return b1Data\n",
    "        \n",
    "def performB1Binning(b1Data, b1Range, b1Stepsize, b1IdentityValue=800):\n",
    "    b1Bins = np.arange(b1Range[0], b1Range[1], b1Stepsize)\n",
    "    b1Clipped = np.clip(b1Data, np.min(b1Bins)*b1IdentityValue, np.max(b1Bins)*b1IdentityValue)\n",
    "    b1Binned = b1Bins[np.digitize(b1Clipped, b1Bins*b1IdentityValue, right=True)]\n",
    "    logging.info(f\"Binned B1 Shape: {np.shape(b1Binned)}\")\n",
    "    return b1Binned\n",
    "\n",
    "def PatternMatchingViaMaxInnerProductWithInterpolation(combined, dictionary, simulation, voxelsPerBatch=500, b1Binned=None, device=None,):\n",
    "    if(device==None):\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "    sizes = np.shape(combined)\n",
    "    numSVDComponents=sizes[0]; matrixSize=sizes[1:4]\n",
    "    patternMatches = np.empty((matrixSize), dtype=DictionaryEntry)\n",
    "    interpolatedMatches = np.empty((matrixSize), dtype=DictionaryEntry)\n",
    "    M0 = torch.zeros((matrixSize), dtype=torch.complex64)\n",
    "    if b1Binned is not None:\n",
    "        for uniqueB1 in np.unique(b1Binned):\n",
    "            logging.info(f\"Pattern Matching B1 Value: {uniqueB1}\")\n",
    "            if uniqueB1 == 0:\n",
    "                patternMatches[b1Binned==uniqueB1] = 0\n",
    "            else:\n",
    "                signalTimecourses = combined[:,b1Binned == uniqueB1]\n",
    "                simulationTimecourses = torch.t(torch.t(torch.tensor(simulation.truncatedResults))[(np.argwhere(dictionary.entries['B1'] == uniqueB1))].squeeze())\n",
    "                dictionaryEntries = dictionary.entries[(np.argwhere(dictionary.entries['B1'] == uniqueB1))]\n",
    "                signalTimecourses = combined[:,b1Binned == uniqueB1]\n",
    "                patternMatches[b1Binned == uniqueB1], interpolatedMatches[b1Binned == uniqueB1], M0[b1Binned == uniqueB1] = BatchPatternMatchViaMaxInnerProductWithInterpolation(signalTimecourses,dictionaryEntries,simulationTimecourses, voxelsPerBatch=voxelsPerBatch, device=device)\n",
    "    else:\n",
    "        signalTimecourses = torch.reshape(combined, (numSVDComponents,-1))\n",
    "        if(dictionary.entries['B1'][0]):\n",
    "            simulationTimecourses = torch.t(torch.t(torch.tensor(simulation.truncatedResults))[(np.argwhere(dictionary.entries['B1'] == 1))].squeeze())\n",
    "            dictionaryEntries = dictionary.entries[(np.argwhere(dictionary.entries['B1'] == 1))]\n",
    "        else:   \n",
    "            simulationTimecourses = torch.tensor(simulation.truncatedResults)\n",
    "            dictionaryEntries = dictionary.entries\n",
    "        patternMatches, interpolatedMatches, M0 = BatchPatternMatchViaMaxInnerProductWithInterpolation(signalTimecourses, dictionaryEntries, simulationTimecourses, voxelsPerBatch=voxelsPerBatch, device=device)\n",
    "    patternMatches = np.reshape(patternMatches, (matrixSize))\n",
    "    interpolatedMatches = np.reshape(interpolatedMatches, (matrixSize))\n",
    "    M0 = np.reshape(M0, (matrixSize)).numpy()\n",
    "    M0 = np.nan_to_num(M0)\n",
    "    return patternMatches, interpolatedMatches, M0\n",
    "\n",
    "# Takes data input as: [cha z y x], [z y x], or [y x]\n",
    "def PopulateISMRMRDImage(header, data, acquisition, image_index, colormap=None, window=None, level=None, comment=\"\"):\n",
    "    image = ismrmrd.Image.from_array(data.transpose(), acquisition=acquisition, transpose=False)\n",
    "    image.image_index = image_index\n",
    "\n",
    "    # Set field of view\n",
    "    image.field_of_view = (ctypes.c_float(header.encoding[0].reconSpace.fieldOfView_mm.x), \n",
    "                            ctypes.c_float(header.encoding[0].reconSpace.fieldOfView_mm.y), \n",
    "                            ctypes.c_float(header.encoding[0].reconSpace.fieldOfView_mm.z))\n",
    "\n",
    "    if colormap is None:\n",
    "        colormap = \"\"\n",
    "    if window is None:\n",
    "        window = np.max(data)\n",
    "    if level is None:\n",
    "        level = np.max(data)/2\n",
    "\n",
    "    # Set ISMRMRD Meta Attributes\n",
    "    meta = ismrmrd.Meta({'DataRole':               'Image',\n",
    "                         'ImageProcessingHistory': ['FIRE', 'PYTHON'],\n",
    "                         'WindowCenter':           str(level),\n",
    "                         'WindowWidth':            str(window), \n",
    "                         'GADGETRON_ColorMap':     colormap,\n",
    "                         'GADGETRON_ImageComment': comment})\n",
    "\n",
    "    # Add image orientation directions to MetaAttributes if not already present\n",
    "    if meta.get('ImageRowDir') is None:\n",
    "        meta['ImageRowDir'] = [\"{:.18f}\".format(image.getHead().read_dir[0]), \"{:.18f}\".format(image.getHead().read_dir[1]), \"{:.18f}\".format(image.getHead().read_dir[2])]\n",
    "\n",
    "    if meta.get('ImageColumnDir') is None:\n",
    "        meta['ImageColumnDir'] = [\"{:.18f}\".format(image.getHead().phase_dir[0]), \"{:.18f}\".format(image.getHead().phase_dir[1]), \"{:.18f}\".format(image.getHead().phase_dir[2])]\n",
    "\n",
    "    xml = meta.serialize()\n",
    "    logging.debug(\"Image MetaAttributes: %s\", xml)\n",
    "    logging.debug(\"Image data has %d elements\", image.data.size)\n",
    "\n",
    "    image.attribute_string = xml\n",
    "    return image\n",
    "\n",
    "def GenerateRadialMask(coilImageData, svdNum = 0, angularResolution = 0.01, stepSize = 3, fillSize = 3, maxDecay = 15, featheringKernelSize=4, coilCountCutoff = 20):\n",
    "    coilMax = np.max(np.abs(coilImageData[svdNum,:,:,:,:].cpu().numpy()), axis=0)\n",
    "    if(np.shape(coilImageData)[1]>coilCountCutoff):\n",
    "        maskIm = np.ones(np.shape(coilMax))\n",
    "        outputMask = np.moveaxis(maskIm, 0,-1)\n",
    "        return outputMask\n",
    "    threshold = np.mean(coilMax)\n",
    "    maskIm = np.zeros(np.shape(coilMax))\n",
    "    center = np.array(np.shape(coilMax)[1:3])/2\n",
    "    Y, X = np.ogrid[:np.shape(coilMax)[2], :np.shape(coilMax)[1]]\n",
    "    dist_from_center = np.sqrt((X - center[0])**2 + (Y-center[1])**2)\n",
    "    cylindricalMask = dist_from_center <= np.shape(coilMax)[1]/2\n",
    "    coilMax = cylindricalMask*coilMax\n",
    "    \n",
    "    for partition in np.arange(0,np.shape(coilMax)[0]):\n",
    "        for polarAngle in np.arange(0,2*np.pi, angularResolution):\n",
    "            decayCounter = 0\n",
    "            radius = 0\n",
    "            historicalPos = []\n",
    "            while decayCounter < maxDecay:\n",
    "                radius += stepSize\n",
    "                pos = (center + [radius*np.cos(polarAngle), radius*np.sin(polarAngle)]).astype(int)\n",
    "                if(pos[0] > 0 and pos[0] < np.shape(coilMax)[1]-1 and pos[1] > 0 and pos[1] < np.shape(coilMax)[2]-1):\n",
    "                    if coilMax[partition,pos[0],pos[1]] > threshold:\n",
    "                        for histPos in historicalPos:\n",
    "                            maskIm[partition, histPos[0]-fillSize:histPos[0]+fillSize, histPos[1]-fillSize:histPos[1]+fillSize] = 1\n",
    "                        historicalPos.clear()\n",
    "                        maskIm[partition, pos[0]-fillSize:pos[0]+fillSize, pos[1]-fillSize:pos[1]+fillSize] = 1\n",
    "                        decayCounter = 0\n",
    "                    else:\n",
    "                        decayCounter += 1\n",
    "                        #maskIm[partition, pos[0]-fillSize:pos[0]+fillSize, pos[1]-fillSize:pos[1]+fillSize] = 1 - (decayCounter/maxDecay)\n",
    "                        historicalPos.append(pos)\n",
    "                else:\n",
    "                     break\n",
    "    device = torch.device(\"cpu\")\n",
    "    maskIm = torch.tensor(maskIm).to(torch.float32)  \n",
    "    meanFilter = torch.nn.Conv3d(in_channels=1, out_channels=1, kernel_size=featheringKernelSize, bias=False, padding='same')\n",
    "    featheringKernelWeights = (torch.ones((featheringKernelSize, featheringKernelSize, featheringKernelSize), \n",
    "                                          dtype=torch.float32)/(featheringKernelSize*featheringKernelSize*featheringKernelSize)).to(device)\n",
    "    meanFilter.weight.data = featheringKernelWeights.unsqueeze(0).unsqueeze(0)\n",
    "    maskIm = meanFilter(maskIm.unsqueeze(0)).squeeze().detach().numpy()\n",
    "    del featheringKernelWeights, meanFilter\n",
    "    outputMask = np.moveaxis(maskIm, 0,-1)\n",
    "    return outputMask\n",
    "\n",
    "# Generate Classification Maps from Timecourses and Known Tissue Timecourses\n",
    "def GenerateClassificationMaps(imageData, dictionary, simulation, matrixSize):\n",
    "    ## Run for all pixels\n",
    "    shape = np.shape(imageData)\n",
    "    timecourses = imageData.reshape(shape[0], -1)\n",
    "\n",
    "    ## Set up coefficient dictionary\n",
    "    coefficientDictionaryEntries = []\n",
    "    stepSize = 1/(10**2)\n",
    "    roundingFactor = 1/stepSize\n",
    "    maxSum = 1\n",
    "\n",
    "    for aValue in np.arange(0, maxSum, stepSize):\n",
    "        remainingForB = maxSum - aValue\n",
    "        if(remainingForB < stepSize):\n",
    "            coefficientDictionaryEntries.append([aValue, remainingForB, 0])\n",
    "        else:   \n",
    "            for bValue in np.arange(0,remainingForB, stepSize):\n",
    "                remainingForC = remainingForB - bValue\n",
    "                coefficientDictionaryEntries.append([aValue, bValue, remainingForC])\n",
    "    coefficientDictionaryEntries = np.array(coefficientDictionaryEntries)\n",
    "    coefficientDictionaryEntries = np.round(coefficientDictionaryEntries*roundingFactor)/roundingFactor\n",
    "    sums = np.sum(coefficientDictionaryEntries, axis=1)\n",
    "    coefficientDictionaryEntries = np.array([tuple(i) for i in coefficientDictionaryEntries], dtype=DictionaryEntry)\n",
    "\n",
    "    ## Timecourse Equation for a voxel\n",
    "    ## Gm(t) = sum across dictionary entries of e^-1*(((T1_gm-T1)/sigmaT1_gm)**2 + ((T2_gm-T2)/sigmaT2_gm)**2)\n",
    "    T1_wm = WHITE_MATTER_3T[0]['T1']; sigmaT1_wm = 0.01\n",
    "    T2_wm = WHITE_MATTER_3T[0]['T2']; sigmaT2_wm = 0.01\n",
    "    T1_gm = GREY_MATTER_3T[0]['T1']; sigmaT1_gm = 0.01\n",
    "    T2_gm = GREY_MATTER_3T[0]['T2']; sigmaT2_gm = 0.01\n",
    "    T1_csf = CSF_3T[0]['T1']; sigmaT1_csf = 0.01\n",
    "    T2_csf = CSF_3T[0]['T2']; sigmaT2_csf = 0.01\n",
    "    T1 = dictionary.entries['T1'][simulation.dictionaryParameters.entries['B1']==1] # Revise to not pass in Dictionary - use the subclass dictionary instead so it matches for sure\n",
    "    T2 = dictionary.entries['T2'][simulation.dictionaryParameters.entries['B1']==1]\n",
    "    WmWeights = np.exp( -1 * ( ((T1_wm - T1)/sigmaT1_wm )**2 + ( (T2_wm-T2)/sigmaT2_wm )**2 ))\n",
    "    GmWeights = np.exp( -1 * ( ((T1_gm - T1)/sigmaT1_gm )**2 + ( (T2_gm-T2)/sigmaT2_gm )**2 )) \n",
    "    CsfWeights = np.exp( -1 * ( ((T1_csf - T1)/sigmaT1_csf )**2 + ( (T2_csf-T2)/sigmaT2_csf )**2 )) \n",
    "\n",
    "    ## Create timecourses for WM/GM/CSF based on the above \n",
    "    truncatedResultsIdentityB1 = simulation.truncatedResults[:,simulation.dictionaryParameters.entries['B1']==1]\n",
    "    WM = np.sum(truncatedResultsIdentityB1* WmWeights, axis=1); GM = np.sum(truncatedResultsIdentityB1 * GmWeights, axis=1); CSF = np.sum(truncatedResultsIdentityB1 * CsfWeights, axis=1)\n",
    "    coefficientDictionaryTimecourses = []\n",
    "    for coefficients in coefficientDictionaryEntries:\n",
    "        coefficientTimecourse = coefficients['T1'] * WM + coefficients['T2'] * GM + coefficients['B1'] * CSF\n",
    "        coefficientDictionaryTimecourses.append(coefficientTimecourse)\n",
    "    coefficientDictionaryTimecourses = np.array(coefficientDictionaryTimecourses).transpose()\n",
    "\n",
    "    # Perform Coefficient-Space Pattern Matching\n",
    "    coefficientPatternMatches,coefficientM0 = BatchedPatternMatchViaMaxInnerProduct(timecourses.to(torch.cfloat), coefficientDictionaryEntries, torch.tensor(coefficientDictionaryTimecourses).to(torch.cfloat))\n",
    "    normalizedCoefficients = coefficientPatternMatches.view((np.dtype('<f4'), len(coefficientPatternMatches.dtype.names)))\n",
    "\n",
    "    wmFractionMap = np.reshape(normalizedCoefficients[:,0], (matrixSize)) * 10000\n",
    "    gmFractionMap = np.reshape(normalizedCoefficients[:,1], (matrixSize)) * 10000\n",
    "    csfFractionMap = np.reshape(normalizedCoefficients[:,2], (matrixSize)) * 10000\n",
    "    \n",
    "    return (wmFractionMap, gmFractionMap, csfFractionMap)\n",
    "\n",
    "def ThroughplaneFFT(nufftResults, device=None):\n",
    "    if(device==None):\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "    sizes = np.shape(nufftResults)\n",
    "    numSVDComponents=sizes[0]; numCoils=sizes[1]; numPartitions=sizes[2]; matrixSize=sizes[3:5]\n",
    "    images = torch.zeros((numSVDComponents, numCoils, numPartitions, matrixSize[0], matrixSize[1]), dtype=torch.complex64)\n",
    "    with tqdm(total=numSVDComponents) as pbar:\n",
    "        for svdComponent in np.arange(0, numSVDComponents):\n",
    "            nufft_device = nufftResults[svdComponent,:,:,:,:].to(device)\n",
    "            images[svdComponent,:,:,:,:] = torch.fft.ifftshift(torch.fft.ifft(nufft_device, dim=1), dim=1)\n",
    "            del nufft_device\n",
    "            pbar.update(1)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Images Shape:\", np.shape(images))   \n",
    "    return images\n",
    "            \n",
    "\n",
    "def runReconstruction(filename, b1Filename=\"\"):\n",
    "    multi_twix = twixtools.read_twix(filename)\n",
    "    numSpirals = int(multi_twix[-1]['hdr']['Meas']['iNoOfFourierLines']); print(f'Spirals: {numSpirals}')\n",
    "    numMeasuredPartitions = int(multi_twix[-1]['hdr']['Meas']['iNoOfFourierPartitions']); print(f'Measured Partitions: {numMeasuredPartitions}')\n",
    "    numUndersampledPartitions = int(multi_twix[-1]['hdr']['MeasYaps']['sKSpace']['lPartitions']); print(f'Undersampled Partitions: {numUndersampledPartitions}')\n",
    "    centerMeasuredPartition =  int(numMeasuredPartitions/2); print(f'Center Measured Partition: {centerMeasuredPartition}') # Fix this to work with partial fourier\n",
    "    numSets = int(multi_twix[-1]['hdr']['Meas']['iNSet']); print(f'Sets: {numSets}')\n",
    "    numCoils = int(multi_twix[-1]['hdr']['Meas']['iMaxNoOfRxChannels']); print(f'Coils: {numCoils}')\n",
    "    xMatSize = multi_twix[-1]['hdr']['MeasYaps']['sKSpace']['lBaseResolution']\n",
    "    yMatSize = multi_twix[-1]['hdr']['MeasYaps']['sKSpace']['lPhaseEncodingLines']\n",
    "    zMatSize = multi_twix[-1]['hdr']['MeasYaps']['sKSpace']['lImagesPerSlab']\n",
    "    matrixSize = np.array([xMatSize, yMatSize, zMatSize]); print(f'Matrix Size: {matrixSize}')\n",
    "    xFOV = multi_twix[-1]['hdr']['MeasYaps']['sSliceArray']['asSlice'][0]['dReadoutFOV']\n",
    "    yFOV = multi_twix[-1]['hdr']['MeasYaps']['sSliceArray']['asSlice'][0]['dPhaseFOV']\n",
    "    zFOV = multi_twix[-1]['hdr']['MeasYaps']['sSliceArray']['asSlice'][0]['dThickness']\n",
    "\n",
    "    undersamplingRatio = 1\n",
    "    if(numUndersampledPartitions > 1): # Hack, may not work for multislice 2d\n",
    "        undersamplingRatio = int(numUndersampledPartitions / (centerMeasuredPartition * 2)); \n",
    "        print(f'Undersampling Ratio: {undersamplingRatio}')\n",
    "    usePartialFourier = False\n",
    "    if(numMeasuredPartitions*undersamplingRatio < numUndersampledPartitions):\n",
    "        usePartialFourier = True\n",
    "        partialFourierRatio = numMeasuredPartitions / (numUndersampledPartitions/undersamplingRatio)\n",
    "        print(f'Measured partitions is less than expected for undersampling ratio - assuming partial fourier acquisition with ratio: {partialFourierRatio}')\n",
    "\n",
    "    # Set up sequence parameter arrays\n",
    "    numTimepoints = numSets*numSpirals\n",
    "    TRs = np.zeros((numTimepoints, numMeasuredPartitions))\n",
    "    TEs = np.zeros((numTimepoints, numMeasuredPartitions))\n",
    "    FAs = np.zeros((numTimepoints, numMeasuredPartitions))\n",
    "    PHs = np.zeros((numTimepoints, numMeasuredPartitions))\n",
    "    IDs = np.zeros((numTimepoints, numMeasuredPartitions))\n",
    "\n",
    "    # Set up raw data and header arrays\n",
    "    rawdata = None\n",
    "    header = ismrmrd.xsd.ismrmrdHeader()\n",
    "    matrixSizeHeader=ismrmrd.xsd.matrixSizeType(xMatSize, yMatSize, zMatSize)\n",
    "    fovHeader=ismrmrd.xsd.fieldOfViewMm(xFOV, yFOV, zFOV)\n",
    "    encoding = ismrmrd.xsd.encodingType(reconSpace=ismrmrd.xsd.encodingSpaceType(matrixSize=matrixSizeHeader, fieldOfView_mm=fovHeader))\n",
    "    header.encoding.append(encoding)\n",
    "    acqHeaders = np.empty((numUndersampledPartitions, numSpirals, numSets), dtype=ismrmrd.Acquisition)\n",
    "    discardPre=0;discardPost=0\n",
    "\n",
    "    ## If dictionary simulation is new, upload to Azure so it will exist forever?\n",
    "    if matrixSize[0]==256:\n",
    "        trajectoryFilepath='mrf_dependencies/trajectories/SpiralTraj_FOV250_256_uplimit1916_norm.bin'\n",
    "        densityFilepath='mrf_dependencies/trajectories/DCW_FOV250_256_uplimit1916.bin'\n",
    "        numToDiscard = 1916\n",
    "    elif matrixSize[0]==400:\n",
    "        trajectoryFilepath='mrf_dependencies/trajectories/SpiralTraj_FOV400_400_uplimit2890_norm.bin'\n",
    "        densityFilepath='mrf_dependencies/trajectories/DCW_FOV400_400_uplimit2890.bin'\n",
    "        numToDiscard = 2890\n",
    "    else:\n",
    "        print('Trajectory unknown, using default')\n",
    "        trajectoryFilepath='mrf_dependencies/trajectories/SpiralTraj_FOV250_256_uplimit1916_norm.bin'\n",
    "        densityFilepath='mrf_dependencies/trajectories/DCW_FOV250_256_uplimit1916.bin'\n",
    "        numToDiscard = 1916\n",
    "\n",
    "    # Process data as it comes in\n",
    "    for mdb in multi_twix[-1]['mdb']:\n",
    "        if mdb is None:\n",
    "            break\n",
    "        if mdb.is_flag_set('NOISEADJSCAN') or mdb.is_flag_set('PHASCOR'):\n",
    "            print('Noise')\n",
    "            continue\n",
    "        else:\n",
    "            acqHeader = ismrmrd.Acquisition()\n",
    "            acqHeader.position[0] = mdb.mdh.SliceData.SlicePos.Sag\n",
    "            acqHeader.position[1] = mdb.mdh.SliceData.SlicePos.Cor\n",
    "            acqHeader.position[2] = mdb.mdh.SliceData.SlicePos.Tra\n",
    "            quat = mdb.mdh.SliceData.Quaternion\n",
    "            a = quat[0]; b = quat[1]; c = quat[2]; d = quat[3]\n",
    "\n",
    "            acqHeader.read_dir[0] = 1.0 - 2.0 * (b * b + c * c)\n",
    "            acqHeader.phase_dir[0] = 2.0 * (a * b - c * d)\n",
    "            acqHeader.slice_dir[0] = 2.0 * (a * c + b * d)\n",
    "            \n",
    "            acqHeader.read_dir[1] = 2.0 * (a * b + c * d)\n",
    "            acqHeader.phase_dir[1] = 1.0 - 2.0 * (a * a + c * c)\n",
    "            acqHeader.slice_dir[1] = 2.0 * (b * c - a * d)\n",
    "            \n",
    "            acqHeader.read_dir[2] = 2.0 * (a * c - b * d)\n",
    "            acqHeader.phase_dir[2] = 2.0 * (b * c + a * d)\n",
    "            acqHeader.slice_dir[2] = 1.0 - 2.0 * (a * a + b * b)\n",
    "            measuredPartition = mdb.mdh.Counter.Par\n",
    "            undersampledPartition = mdb.mdh.IceProgramPara[1]\n",
    "            spiral = mdb.mdh.Counter.Lin\n",
    "            set = mdb.mdh.Counter.Set\n",
    "            timepoint = mdb.mdh.IceProgramPara[0]\n",
    "            TRs[timepoint, measuredPartition] = mdb.mdh.IceProgramPara[2]          \n",
    "            TEs[timepoint, measuredPartition] = mdb.mdh.IceProgramPara[3]\n",
    "            FAs[timepoint, measuredPartition] = mdb.mdh.IceProgramPara[4]  # Use requested FA\n",
    "            #FAs[timepoint, measuredPartition] = acq.user_int[5] # Use actual FA not requested\n",
    "            PHs[timepoint, measuredPartition] = mdb.mdh.IceProgramPara[6] \n",
    "            IDs[timepoint, measuredPartition] = spiral\n",
    "            #print(undersampledPartition, spiral, set)\n",
    "            acqHeaders[undersampledPartition, spiral, set] = acqHeader\n",
    "            if rawdata is None:\n",
    "                discardPre = int(mdb.mdh.CutOff.Pre / 2); print(f'Discard Pre: {discardPre}') # Fix doubling in sequence - weird;\n",
    "                discardPost = discardPre + numToDiscard; print(f'Discard Post: {discardPost}') # Fix in sequence\n",
    "                numReadoutPoints = discardPost-discardPre; print(f'Readout Points: {numReadoutPoints}')\n",
    "                rawdata = np.zeros([numCoils, numUndersampledPartitions, numReadoutPoints, numSpirals, numSets], dtype=np.complex64)\n",
    "            readout = mdb.data[:, discardPre:discardPost]\n",
    "            rawdata[:, undersampledPartition, :, spiral, set] = readout\n",
    "    \n",
    "    B1map = LoadB1Map(matrixSize, b1Filename)\n",
    "    if(np.size(B1map)!=0):\n",
    "        B1map_binned = performB1Binning(B1map, b1Range, b1Stepsize, b1IdentityValue=800)\n",
    "        dictionary = DictionaryParameters.GenerateFixedPercent(dictionaryName, percentStepSize=percentStepSize, t1Range=t1Range, t2Range=t2Range, includeB1=True, b1Range=b1Range, b1Stepsize=b1Stepsize)\n",
    "    else:\n",
    "        B1map_binned = None\n",
    "        dictionary = DictionaryParameters.GenerateFixedPercent(dictionaryName, percentStepSize=percentStepSize, t1Range=t1Range, t2Range=t2Range, includeB1=False, b1Range=None, b1Stepsize=None)\n",
    "\n",
    "    ## Initialize the Sequence\n",
    "    sequence = SequenceParameters(\"largescale\", SequenceType.FISP)\n",
    "    print(\"TRs:\", np.min(TRs), np.max(TRs))\n",
    "    print(\"TEs:\", np.min(TEs), np.max(TEs))\n",
    "    print(\"FAs:\", np.min(FAs), np.max(FAs))\n",
    "    print(\"IDs:\", np.min(IDs), np.max(IDs))\n",
    "    sequence.Initialize(TRs[:,0]/(1000*1000), TEs[:,0]/(1000*1000), FAs[:,0]/(100), PHs[:,0]/(100), IDs[:,0])\n",
    "    simulation = Simulation(sequence, dictionary, phaseRange=phaseRange, numSpins=numSpins)\n",
    "    simulationHash = hashlib.sha256(pickle.dumps(simulation)).hexdigest()\n",
    "    if(dictionaryFolder == \"\"):\n",
    "        dictionaryPath = simulationHash+\".simulation\"\n",
    "    else:\n",
    "        dictionaryPath = dictionaryFolder+\"/\"+simulationHash+\".simulation\"\n",
    "    logging.info(f\"Dictionary Path: {dictionaryPath}\")\n",
    "    Path(dictionaryFolder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ## Check if dictionary already exists\n",
    "    if (os.path.isfile(dictionaryPath)):\n",
    "        logging.info(\"Dictionary already exists. Using local copy.\")\n",
    "        filehandler = open(dictionaryPath,'rb')\n",
    "        simulation = pickle.load(filehandler) \n",
    "        filehandler.close()\n",
    "\n",
    "    else:        \n",
    "        ## Simulate the Dictionary\n",
    "        logging.info(\"Dictionary not found. Simulating. \")\n",
    "        simulation.Execute(numBatches=numBatches)\n",
    "        simulation.CalculateSVD(truncationNumberOverride=10)\n",
    "        logging.info(f\"Simulated {numSpirals*numSets} timepoints\")\n",
    "        del simulation.results\n",
    "        filehandler = open(dictionaryPath, 'wb')\n",
    "        pickle.dump(simulation, filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "    ## Run the Reconstruction\n",
    "    svdData = ApplySVDCompression(rawdata, simulation, device=torch.device(\"cpu\"))\n",
    "    (trajectoryBuffer,trajectories,densityBuffer,_) = LoadSpirals(trajectoryFilepath, densityFilepath, numSpirals)\n",
    "    svdData = ApplyXYZShift(svdData, header, acqHeaders, trajectories)\n",
    "    nufftResults = PerformNUFFTs(svdData, trajectoryBuffer, densityBuffer, matrixSize, matrixSize*2)\n",
    "    del svdData\n",
    "    coilImageData = ThroughplaneFFT(nufftResults)\n",
    "    del nufftResults\n",
    "    imageData, coilmaps = PerformWalshCoilCombination(coilImageData)\n",
    "    imageMask = GenerateRadialMask(coilImageData, coilCountCutoff=0)\n",
    "    patternMatchResults, interpolatedResults, M0 = PatternMatchingViaMaxInnerProductWithInterpolation(imageData, dictionary, simulation, b1Binned = B1map_binned, voxelsPerBatch=2000)\n",
    "    (wmFractionMap, gmFractionMap, csfFractionMap) = GenerateClassificationMaps(imageData, dictionary, simulation, matrixSize)\n",
    "    reconstructionFinishTime = time.time()\n",
    "\n",
    "    import imageio\n",
    "    saveDir = \"exports\"\n",
    "    os.makedirs(saveDir,exist_ok=True)\n",
    "    T1map_interp = AddText((imageMask>0.1) * interpolatedResults['T1'] * 1000) # to milliseconds\n",
    "    T2map_interp = AddText((imageMask>0.1) * interpolatedResults['T2'] * 1000) # to milliseconds\n",
    "    M0map = AddText((imageMask>0.1) * (np.abs(M0) / np.max(np.abs(M0))) * 2**12)\n",
    "    WMmap = AddText((imageMask>0.1) * wmFractionMap)\n",
    "    GMmap = AddText((imageMask>0.1) * gmFractionMap)\n",
    "    CSFmap = AddText((imageMask>0.1) * csfFractionMap)\n",
    "    for slice in np.arange(0,np.shape(patternMatchResults)[2]):\n",
    "        imageio.imwrite(saveDir + \"t1_\" + str(slice)  + '.tif', T1map_interp[:,:,slice])\n",
    "        imageio.imwrite(saveDir + \"t2_\" + str(slice)  + '.tif', T2map_interp[:,:,slice])\n",
    "        imageio.imwrite(saveDir + \"m0_\" + str(slice)  + '.tif', M0map[:,:,slice])\n",
    "        imageio.imwrite(saveDir + \"wm_\" + str(slice)  + '.tif', WMmap[:,:,slice])\n",
    "        imageio.imwrite(saveDir + \"gm_\" + str(slice)  + '.tif', GMmap[:,:,slice])\n",
    "        imageio.imwrite(saveDir + \"csf_\" + str(slice)  + '.tif', CSFmap[:,:,slice])\n",
    "\n",
    "root = tk.Tk()\n",
    "def select_file():\n",
    "    filetypes = (\n",
    "        ('Siemens Raw Data', '*.dat'),\n",
    "        ('All files', '*.*')\n",
    "    )\n",
    "\n",
    "    filename = fd.askopenfilename(\n",
    "        title='Open a file',\n",
    "        initialdir='/',\n",
    "        filetypes=filetypes)\n",
    "\n",
    "    showinfo(\n",
    "        title='',\n",
    "        message=\"Beginning Reconstruction: \" + filename\n",
    "    )\n",
    "\n",
    "    runReconstruction(filename)\n",
    "    root.destroy()\n",
    "\n",
    "\n",
    "# create the root window\n",
    "root.title('Open File Dialog')\n",
    "root.resizable(False, False)\n",
    "root.geometry('300x150')\n",
    "\n",
    "# open button\n",
    "open_button = ttk.Button(\n",
    "    root,\n",
    "    text='Select .dat File',\n",
    "    command=select_file\n",
    ")\n",
    "open_button.pack(expand=True)\n",
    "\n",
    "# run the application\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
